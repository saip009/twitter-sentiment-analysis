{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This short tutorial will not only guide you through some basic data analysis methods but it will also show you how to implement some of the more sophisticated techniques available today. We will look into traffic accident data from the National Highway Traffic Safety Administration and try to predict fatal accidents using state-of-the-art statistical learning techniques.  If you are interested, download the code at the bottom and follow along as we work through a real world data set. This post is in Python while a companion post covers the same techniques in R.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## First things first\n",
      "For those of you who are not familiar with Python and some of its most popular libraries for data science, please follow along with [this blogpost](http://www.datarobot.com/blog/getting-up-and-running-with-python \"Get IPython\"), which will get you set up with an environment similar to the one we will be using.  There are instructions for Mac, Linux, and Windows environments, so hopefully we have all the bases covered.\n",
      "\n",
      "IPython is awesome, as you will come to find out.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Get some data\n",
      "Being able to play with data requires having the data itself, so let's take care of that right now.  The National Highway Traffic Safety Administration (NHTSA) has some really cool data that they make public.  The following code snippet will take care of downloading the data to a new directory, and extracting the files from that zipfile.  The zip is 14.9 MB so it might take some time to run - it is worth the wait! This is really cool data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import zipfile\n",
      "import urllib2\n",
      "import os\n",
      "\n",
      "source_url = 'ftp://ftp.nhtsa.dot.gov/GES/GES12/GES12_Flatfile.zip'\n",
      "zip_name = 'GES12_Flatfile.zip'\n",
      "cwd = os.getcwd()\n",
      "dir_path  = os.path.join(cwd, 'GES2012')\n",
      "zip_path = os.path.join(dir_path, zip_name)\n",
      "\n",
      "# We'll make a directory for you to play around with,\n",
      "# then when you're done playing you can just delete the directory\n",
      "if not os.path.exists(dir_path):\n",
      "    os.makedirs(dir_path)\n",
      "\n",
      "# Download the file from GES website if you haven't already\n",
      "if not os.path.exists(zip_path):\n",
      "    response = urllib2.urlopen(source_url)\n",
      "    with open(zip_path, 'wb') as fh:\n",
      "        x = response.read()\n",
      "        fh.write(x)\n",
      "\n",
      "# Extract all the files from that zipfile\n",
      "with zipfile.ZipFile(os.path.join(dir_path, zip_name), 'r') as z:\n",
      "    z.extractall(dir_path)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#See what we just unzipped\n",
      "os.listdir(dir_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['VIOLATN.TXT',\n",
        " 'DRIMPAIR.TXT',\n",
        " 'VEHICLE.TXT',\n",
        " 'PERSON.TXT',\n",
        " 'VSOE.TXT',\n",
        " 'PARKWORK.TXT',\n",
        " '2012GESFlatFileTXT.sas',\n",
        " 'GES12_Flatfile.zip',\n",
        " 'NMPRIOR.TXT',\n",
        " 'VEVENT.TXT',\n",
        " 'DISTRACT.TXT',\n",
        " 'CEVENT.TXT',\n",
        " 'DAMAGE.TXT',\n",
        " 'ACCIDENT.TXT',\n",
        " 'SAFETYEQ.TXT',\n",
        " 'VISION.TXT',\n",
        " 'NMIMPAIR.TXT',\n",
        " 'FACTOR.TXT',\n",
        " 'MANEUVER.TXT',\n",
        " 'NMCRASH.TXT']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load the data into Python\n",
      "\n",
      "With our data downloaded and readily accessible, we can start to play around and see what we can learn from the data.  Many of the columns have an encoding that you will need to read [the manual](ftp://ftp.nhtsa.dot.gov/GES/GES12/GES%20Analytical%20Users%20Manual%201988-2012_FINAL-2013-10-31.pdf \"GES 2012 Manual\") in order to understand, so it might be useful to download that PDF so you can easily refer to it.  We will be looking at ``PERSON.TXT``, which contains information at the level of the individuals involved in the accidents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn\n",
      "\n",
      "cwd = os.getcwd()\n",
      "dir_path  = os.path.join(cwd, 'GES2012')\n",
      "input_file_path = os.path.join(dir_path, 'PERSON.TXT')\n",
      "\n",
      "input_data = pd.read_csv(input_file_path, delimiter='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(input_data.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['AGE',\n",
        " 'AGE_IM',\n",
        " 'AIR_BAG',\n",
        " 'ALC_RES',\n",
        " 'ALC_STATUS',\n",
        " 'ATST_TYP',\n",
        " 'BODY_TYP',\n",
        " 'CASENUM',\n",
        " 'DRINKING',\n",
        " 'DRUGRES1',\n",
        " 'DRUGRES2',\n",
        " 'DRUGRES3',\n",
        " 'DRUGS',\n",
        " 'DRUGTST1',\n",
        " 'DRUGTST2',\n",
        " 'DRUGTST3',\n",
        " 'DSTATUS',\n",
        " 'EJECTION',\n",
        " 'EJECT_IM',\n",
        " 'EMER_USE',\n",
        " 'FIRE_EXP',\n",
        " 'HARM_EV',\n",
        " 'HOSPITAL',\n",
        " 'HOUR',\n",
        " 'IMPACT1',\n",
        " 'INJSEV_IM',\n",
        " 'INJ_SEV',\n",
        " 'LOCATION',\n",
        " 'MAKE',\n",
        " 'MAN_COLL',\n",
        " 'MINUTE',\n",
        " 'MOD_YEAR',\n",
        " 'MONTH',\n",
        " 'PERALCH_IM',\n",
        " 'PER_NO',\n",
        " 'PER_TYP',\n",
        " 'PJ',\n",
        " 'PSU',\n",
        " 'PSUSTRAT',\n",
        " 'P_SF1',\n",
        " 'P_SF2',\n",
        " 'P_SF3',\n",
        " 'REGION',\n",
        " 'REST_MIS',\n",
        " 'REST_USE',\n",
        " 'ROLLOVER',\n",
        " 'SCH_BUS',\n",
        " 'SEAT_IM',\n",
        " 'SEAT_POS',\n",
        " 'SEX',\n",
        " 'SEX_IM',\n",
        " 'SPEC_USE',\n",
        " 'STRATUM',\n",
        " 'STR_VEH',\n",
        " 'TOW_VEH',\n",
        " 'VEH_NO',\n",
        " 'VE_FORMS',\n",
        " 'WEIGHT']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Clean up the data\n",
      "One prediction task you might find interesting is predicting whether or not a crash was fatal.  The column ``INJSEV_IM`` contains imputed values for the severity of the injury, but there are still one value that might complicate analysis - level 6 indicates that the person died prior to the crash."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_data.INJSEV_IM.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0    100840\n",
        "2     20758\n",
        "1     19380\n",
        "3      9738\n",
        "5      1179\n",
        "4      1178\n",
        "6         4\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Forntunately, there are only four of those cases within the dataset, so it is not unreasonable to ignore them during our analysis.  However, we will find that a few of the columns in the data have missing values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Drop those odd cases\n",
      "input_data = input_data[input_data.INJSEV_IM != 6]\n",
      "\n",
      "for column_name in input_data.columns:\n",
      "    n_nans = input_data[column_name].isnull().sum()\n",
      "    if n_nans > 0:\n",
      "        print column_name, n_nans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MAKE 5162\n",
        "BODY_TYP 5162\n",
        "MOD_YEAR 5162\n",
        "TOW_VEH 5162\n",
        "SPEC_USE 5162\n",
        "EMER_USE 5162\n",
        "ROLLOVER 5162\n",
        "IMPACT1 5162\n",
        "FIRE_EXP 5162\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this analysis, we will just drop these rows (they are all the same rows) - but you certainly don't have to do that.  In fact, maybe there is a systematic data entry error that is causing them to be interpreted incorrectly.  Regardless of the way you cleanup this data, we will most assuredly want to drop the column ``INJ_SEV``, as it is the non-imputed version of ``INJSEV_IM`` and is a pretty severe data leak - there are others as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print input_data.shape\n",
      "data = input_data[~input_data.MAKE.isnull()]\n",
      "discarded = data.pop('INJ_SEV')\n",
      "target = data.pop('INJSEV_IM')\n",
      "print data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(153073, 58)\n",
        "(147911, 56)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One more preprocessing step we'll do is to transform the response.  If you flip to the manual it shows that category ``4`` is a fatal injury - so we will encode our target as such."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target = (target == 4).astype('float')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Now we model!\n",
      "We have predictors, we have a target, now it is time to build a model.  We will be using ordinary least squares, a [Ridge Regression](http://en.wikipedia.org/wiki/Ridge_regression \"wikipedia\") and [Lasso Regression](http://en.wikipedia.org/wiki/Lasso_%28statistics%29#Lasso_method \"wikipedia\"), both being forms of regularized Linear Regression, \n",
      "[Gradient Boosting Machine](http://en.wikipedia.org/wiki/Gradient_boosting \"wikipedia\") (GBM) and a [CART](http://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees \"wikipedia\") to have some variety in modeling methods.  These are just some representatives from the [scikit-learn](scikit-learn.org/stable/modules/classes.html \"scikit-learn documentation\") library, which gives access to quite a few machine learning techniques.\n",
      "\n",
      "Don't be alarmed if these cell blocks take quite a bit of time to run - the data is of non-negligible size.  Additionally, some of the models perform a search over several parameters to find a best fit, and the gradient boosting classifier is building many trees in order to produce its ensembled decisions.  There is a lot of computation going on under the hood, so get up and take a break if you need."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# Train on half of the data while reserving the other half for\n",
      "# model comparisons\n",
      "xtrain, xtest, ytrain, ytest = sklearn.cross_validation.train_test_split(\n",
      "    data.values, target.values, train_size=0.5)\n",
      "\n",
      "linreg = LinearRegression()\n",
      "linreg.fit(xtrain, ytrain)\n",
      "\n",
      "lr_preds = linreg.predict(xtest)\n",
      "lr_perf = roc_auc_score(ytest, lr_preds)\n",
      "print 'OLS: Area under the ROC curve = {}'.format(lr_perf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OLS: Area under the ROC curve = 0.935139729907\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "ridge = GridSearchCV(Ridge(),\n",
      "                     {'alpha': np.logspace(-10, 10, 10)})\n",
      "ridge.fit(xtrain, ytrain)\n",
      "ridge_preds = ridge.predict(xtest)\n",
      "ridge_performance = roc_auc_score(ytest, ridge_preds)\n",
      "print 'Ridge: Area under the ROC curve = {}'.format(ridge_performance)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ridge: Area under the ROC curve = 0.935221465912\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Lasso\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "lasso = GridSearchCV(Lasso(),\n",
      "                     {'alpha': np.logspace(-10, -8, 5)})\n",
      "lasso.fit(xtrain, ytrain)\n",
      "lasso_preds = lasso.predict(xtest)\n",
      "lasso_performance = roc_auc_score(ytest, lasso_preds)\n",
      "print 'Lasso: Area under the ROC curve = {}'.format(lasso_performance)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lasso: Area under the ROC curve = 0.939851198289\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "gbm = GradientBoostingClassifier(n_estimators=500)\n",
      "\n",
      "gbm.fit(xtrain, ytrain)\n",
      "gbm_preds = gbm.predict_proba(xtest)[:, 1]\n",
      "gbm_performance = roc_auc_score(ytest, gbm_preds)\n",
      "\n",
      "print 'GBM: Area under the ROC curve = {}'.format(gbm_performance)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GBM: Area under the ROC curve = 0.970431372668\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "tree = GridSearchCV(DecisionTreeClassifier(),\n",
      "                    {'max_depth': np.arange(3, 10)})\n",
      "\n",
      "tree.fit(xtrain, ytrain)\n",
      "tree_preds = tree.predict_proba(xtest)[:, 1]\n",
      "tree_performance = roc_auc_score(ytest, tree_preds)\n",
      "\n",
      "print 'DecisionTree: Area under the ROC curve = {}'.format(tree_performance)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTree: Area under the ROC curve = 0.913468225877\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As one final morsel for you to chew on, it would be good to understand which variables the GBM model thinks are most useful for classification.  Spoiler alert: data leaks ahead."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "importances = pd.Series(gbm.feature_importances_, index=data.columns)\n",
      "print importances.order(ascending=False)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "STRATUM       0.261623\n",
        "REST_USE      0.123142\n",
        "EJECT_IM      0.071433\n",
        "AGE_IM        0.056080\n",
        "ALC_RES       0.050908\n",
        "AIR_BAG       0.048473\n",
        "HARM_EV       0.044688\n",
        "HOUR          0.043379\n",
        "DRUGS         0.042505\n",
        "PERALCH_IM    0.035529\n",
        "dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Now what should I do?\n",
      "We have a great [blogpost](http://www.datarobot.com/blog/regularized-linear-regression-with-scikit-learn \"Sage Wisdom\") that\n",
      "goes into more detail about regularized linear regression, if that is what you are interested in.  It would also be good to look into all the models that are offered by [scikit-learn](http://scikit-learn.org) - you might find some you have never heard of!  Beyond that, here are a few challenges that you can undertake to help you hone your data science skills.\n",
      "\n",
      "### Data Prep\n",
      "If it wasn't obvious in the blog post, the column ``STRATUM`` is a data leak (it encodes the severity of the crash).  Which other columns contain data leaks?  Can you come up with a rigorous method to generate candidates for deletion without having to read the entire GES manual?\n",
      "\n",
      "And while we are considering data preparation, consider the column ``REGION``.  Any regression model will consider the West region to be 4 times more ``REGION``-y than the Northeast - that just doesn't make sense.  Which columns could benefit from a [one-hot encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)?\n",
      "\n",
      "### Which is the best model?\n",
      "How good of a model can you build for predicting fatalities from car crashes?  First you will need to settle on a metric of \"good\" - and be prepared to reason why it is a good metric.  How bad is it to be wrong?  How good is it to be right?\n",
      "\n",
      "In order to avoid [overfitting](http://en.wikipedia.org/wiki/Overfitting) you will want to separate some of the data and hold it in reserve for when you evaluate your models - some of these models are expressive enough to memorize all the data!\n",
      "\n",
      "### Which is the best story?\n",
      "Of course, data science is more than just gathering data and building models - it's about telling a story backed-up by the data.  Do crashes with alcohol involved tend to lead to more serious injuries?  When it is late at night, are there more convertibles involved in crashes than other types of vehicles (this one involves looking at a different dataset with the GES data)?  Which is the safest seat in a car?  And how sure can you be that your findings are statistically relevant?\n",
      "\n",
      "Good luck coming up with a great story!\n",
      "\n",
      "\n",
      "*This post was written by Dallin Akagi and Mark Steadman.  Please post any feedback, comments, or questions below or send us an email at &lt;firstname&gt;@datarobot.com.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}